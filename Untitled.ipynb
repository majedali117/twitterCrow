{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\majed\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:3795: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete [[3 2 3 ... 3 3 3]\n",
      " [2 2 3 ... 3 3 1]\n",
      " [3 2 3 ... 3 3 1]\n",
      " ...\n",
      " [3 3 3 ... 3 3 2]\n",
      " [1 3 2 ... 1 3 1]\n",
      " [3 3 3 ... 3 3 1]]\n",
      "Real Reliability of annotators\n",
      "[['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'u' 'u' 'r' 'u' 'r'\n",
      "  'r' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'u'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['u' 'u' 'u' 'u' 'r' 'u' 'r' 'u' 'r' 'u' 'u' 'r' 'u' 'u' 'u' 'r' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'u'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'u' 'u' 'r' 'u' 'u' 'u' 'r' 'u' 'u'\n",
      "  'r' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'r' 'r' 'u' 'u' 'u' 'r' 'r' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'r' 'r' 'u' 'u' 'u' 'u' 'u' 'u' 'r'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']]\n",
      "Estimated Reliability of annotators\n",
      "[['r' 'u' 'r' 'r' 'u' 'r' 'u' 'u' 'r' 'u' 'u' 'r' 'r' 'r' 'u' 'u' 'u' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'u' 'r' 'r' 'r' 'r' 'u' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['u' 'r' 'r' 'r' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'u' 'u' 'u' 'r' 'u' 'r' 'r' 'r' 'u' 'u' 'u' 'r' 'r'\n",
      "  'u' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'r' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'r' 'u' 'u' 'r' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'u' 'r' 'r' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'r' 'u' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'u' 'u' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r' 'u' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'u' 'r' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'r' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'r' 'r' 'r' 'u' 'u' 'u' 'u' 'r' 'u' 'r' 'r' 'u' 'u' 'u' 'u' 'r' 'r'\n",
      "  'u' 'u']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['s' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n",
      "  's' 's']\n",
      " ['u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']\n",
      " ['r' 'u' 'r' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'r' 'u' 'u' 'u' 'u' 'u' 'r'\n",
      "  'u' 'u']\n",
      " ['r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r' 'r'\n",
      "  'r' 'r']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets 14640\n",
      "number of topics 20\n",
      "number of annotators 50\n",
      "accuracy of inter-agreement with topics 0.6969262295081967 missclasification:  4437 of 14640\n",
      "accuracy of Kappa inter-agreement topics 0.696584699453552 missclasification: 4442 of 14640\n",
      "accuracy of inter-agreement without topics 0.6968579234972677 missclasification: 4438 of 14640\n",
      "accuracy of Kappa inter-agreement without topics 0.6984289617486339 missclasification: 4415 of 14640\n",
      "accuracy of inter-agreement then reliability 0.6968579234972677 missclasification:  4438 of 14640\n",
      "accuracy of kappa inter-agreement then reliability 0.6968579234972677 missclasification:  4438 of 14640\n",
      "accuracy of Majority Voting 0.6984289617486339 missclasification: 4415 of 14640\n",
      "accuracy of annotators reliability 0.505 missclasification: 495 of 1000\n",
      "sparsity of the responses matrix 0.0 empty: 0 of  732000\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun  6 14:26:57 2018\n",
    "\n",
    "@author: HP\n",
    "\"\"\"\n",
    "\n",
    "#================================Topic Modelling============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.decomposition import NMF\n",
    "from array import array\n",
    "\n",
    " \n",
    "dataset = pd.read_csv(\"Tweets.csv\")\n",
    "data = dataset['text']\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=20, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "# apply NMF\n",
    "nmf = NMF(n_components=20, init='random', tol=1, random_state=0, max_iter = 200)\n",
    "W = nmf.fit_transform(X)\n",
    "tweets_topics=W\n",
    "S=W\n",
    "sum_rows=W.sum(axis=1)\n",
    "\n",
    "for i in range(len(W)):\n",
    " for j in range(len(W[i])):\n",
    "   tweets_topics[i][j]=round(W[i,j]/sum_rows[i],3)\n",
    "data_temp=data[:10]\n",
    "\n",
    "\n",
    "'''tweets_topics=np.zeros((10,20))   \n",
    "for i in range(10):\n",
    " for j in range(len(W[i])):\n",
    "   tweets_topics[i][j]=round(W[i,j]/sum_rows[i],3)\n",
    "'''\n",
    "H = nmf.components_\n",
    "#=============================End Topic Modelling==================================\n",
    "\n",
    "#===================================Ground Truth====================================\n",
    "import pandas as pd\n",
    "import re\n",
    "dataset = pd.read_csv(\"Tweets.csv\")\n",
    "documents = dataset[['airline_sentiment','text']]\n",
    "documents.replace({'neutral': 1, 'positive': 2, 'negative': 3}, inplace=True)\n",
    "groundTruth=documents['airline_sentiment']\n",
    "groundTruth_temp=groundTruth[:10]\n",
    "#================================End Ground Truth===================================\n",
    "\n",
    "#==============================Simulate labels===========================================\n",
    "from random import randrange, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm\n",
    "import surprise\n",
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "import pandas as pd \n",
    "from scipy.spatial import distance\n",
    "from surprise.model_selection import cross_validate\n",
    "import random\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "np_tweets=len(tweets_topics)\n",
    "np_topics=len(tweets_topics[0])\n",
    "nb_annotators=50\n",
    "nb_labels=3\n",
    "x=[]\n",
    "likelihood=[]\n",
    "annt_responses=np.full((nb_annotators,np_tweets),0)\n",
    "annt_topics=np.full((nb_annotators,np_topics),1.0)\n",
    "#===========generate labels===============================       \n",
    "topics=[]\n",
    "topics=np.zeros(np_topics)\n",
    "\n",
    "for i in range(0,np_topics):\n",
    "    for j in range(0,np_tweets):\n",
    "        topics[i]=topics[i]+tweets_topics[j,i]\n",
    "    \n",
    "for m in range(0,nb_annotators):\n",
    " done=False\n",
    " while(done==False):\n",
    "   val=np.random.normal(0.6, 0.2,1)#the accurcy for each annotater\n",
    "   if val>0 and val<1:\n",
    "       x.append(val)\n",
    "       done=True\n",
    "\n",
    "for m in range(0,nb_annotators):\n",
    " done=False\n",
    " while(done==False):\n",
    "   val=np.random.normal(0.5, 0.1,1)#the Likelihood of response for each annotater\n",
    "   if val>0.0 and val<1.0:\n",
    "       likelihood.append(val)\n",
    "       done=True\n",
    "for m in range(0,nb_annotators):\n",
    "  done=False\n",
    "  while(done==False):\n",
    "   if x[m]>0.0 and x[m]<1.0:\n",
    "    for i in range(0,np_tweets):\n",
    "        correct=np.random.binomial(1,x[m],1)\n",
    "        annotate=np.random.binomial(1,likelihood[m],1)\n",
    "        if (annotate[0]!=0.0):\n",
    "         if correct[0]==1:   \n",
    "          annt_responses[m,i]=groundTruth[i]\n",
    "          for c in range(0,np_topics):\n",
    "                if tweets_topics[i,c]!=0.0:\n",
    "                    annt_topics[m,c]=round(annt_topics[m,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "         else:\n",
    "           annt_responses[m,i]=randrange(1,nb_labels+1,1)  \n",
    "           if annt_responses[m,i]==groundTruth[i]:\n",
    "               for c in range(0,np_topics):\n",
    "                if tweets_topics[i,c]!=0.0:\n",
    "                    annt_topics[m,c]=round(annt_topics[m,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "           else:\n",
    "               for c in range(0,np_topics):\n",
    "                if tweets_topics[i,c]!=0.0:\n",
    "                    annt_topics[m,c]=round(annt_topics[m,c]-(tweets_topics[i,c]/topics[c]),4)\n",
    "             \n",
    "   done=True\n",
    "#=========================End generate label==========================\n",
    "#==============================End simulate labels=========================================\n",
    "#====================Annotators as Classifiers==============================================\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "'''parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3),\n",
    " }'''\n",
    "\n",
    "for i in range(0,nb_annotators):\n",
    "    trainingSet=[]\n",
    "    trainingTarget=[]\n",
    "    test=[]\n",
    "    for j in range(0,np_tweets):\n",
    "         if annt_responses[i][j]!= 0:\n",
    "            trainingSet.append(data[j])\n",
    "            trainingTarget.append(groundTruth[j])\n",
    "    text_clf = Pipeline([('vect', TfidfVectorizer(stop_words='english')),('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "    text_clf = text_clf.fit(trainingSet, trainingTarget)\n",
    "    #gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    #gs_clf = gs_clf.fit(trainingSet, trainingTarget)\n",
    "    #gs_clf.best_score_\n",
    "    #gs_clf.best_params_\n",
    "    for j in range(0,np_tweets):\n",
    "         if annt_responses[i][j]== 0:\n",
    "           test=[]\n",
    "           test.append(data[j])\n",
    "           predicted = text_clf.predict(test)\n",
    "           annt_responses[i][j]=predicted[0]\n",
    "print('complete',annt_responses)\n",
    "#====================End annotators as classifiers=======================================   \n",
    "#==========================inter-agreement===========================================\n",
    "annt_tpc=np.full((nb_annotators,np_topics),1.0)\n",
    "agree=[]\n",
    "totalsim=0\n",
    "trueLabels=[]\n",
    "for i in range(0,nb_annotators):\n",
    " agreement=0\n",
    " for j in range(0,nb_annotators):\n",
    "    for z in range(0,np_tweets):\n",
    "      if annt_responses[i][z]!=0:\n",
    "       if annt_responses[i][z]==annt_responses[j][z] and i!=j:\n",
    "         agreement=agreement+1\n",
    " agree.append(agreement/(nb_annotators-1))\n",
    " totalsim=totalsim+agreement/(nb_annotators-1)\n",
    "#===============End inter-agreement==================================\n",
    "#=============== Kappa inter-agreement=============================\n",
    "kappa_agree=[]\n",
    "for i in range(0,nb_annotators):\n",
    " norm=0\n",
    " for j in range(0,nb_annotators):\n",
    "    kappa=0.0\n",
    "    common=False\n",
    "    confusion=np.full((nb_labels,nb_labels),0)\n",
    "    for z in range(0,np_tweets):\n",
    "        for l in range(1,nb_labels+1):\n",
    "           if annt_responses[i][z]!=0 and annt_responses[j][z]!=0:\n",
    "               common=True \n",
    "               confusion[(annt_responses[i][z])-1][(annt_responses[j][z])-1]=confusion[(annt_responses[i][z])-1][(annt_responses[j][z])-1]+1\n",
    "    if common==True:\n",
    "        norm=norm+1\n",
    "    total=confusion.sum()\n",
    "    pra=np.trace(confusion)/total\n",
    "    pre=0.0\n",
    "    cols=confusion.sum(axis=0)\n",
    "    rows=confusion.sum(axis=1)\n",
    "    for i in range(0,nb_labels):\n",
    "      pre=pre+(cols[i]*rows[i])/total\n",
    "    pre=pre/total\n",
    "    kappa=kappa+((pra-pre)/(1-pre))\n",
    " kappa_agree.append(kappa/(norm))\n",
    " \n",
    "#===============End Kappa inter- agreement========================\n",
    "#=====================inter agreement with topics\n",
    "\n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+(agree[j]*annt_tpc[j,c])\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " trueLabels.append(truelabel)\n",
    " for j in range(0,nb_annotators):\n",
    "  if (annt_responses[j][i]==trueLabels[i]) :\n",
    "          for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc[j,c]=round(annt_tpc[j,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "  else:\n",
    "         for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc[j,c]=round(annt_tpc[j,c]-(tweets_topics[i,c]/topics[c]),4)\n",
    "#====================End inter agreement with topics=================================================\n",
    "\n",
    "#============Kappa with topics============================\n",
    "annt_tpc_kappa=np.full((nb_annotators,np_topics),1.0)\n",
    "Kappa_trueLabels=[]\n",
    "\n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+(kappa_agree[j]*annt_tpc_kappa[j,c])\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " Kappa_trueLabels.append(truelabel)\n",
    " for j in range(0,nb_annotators):\n",
    "  if (annt_responses[j][i]==trueLabels[i]) :\n",
    "          for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc_kappa[j,c]=round(annt_tpc_kappa[j,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "  else:\n",
    "         for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc_kappa[j,c]=round(annt_tpc_kappa[j,c]-(tweets_topics[i,c]/topics[c]),4)\n",
    "    \n",
    "#==============End Kappa with topics=================================\n",
    "#=======================inter agreement without topics===============================\n",
    "trueLabelsWithoutTopics=[]\n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+agree[j]\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " trueLabelsWithoutTopics.append(truelabel)\n",
    "\n",
    "#======================End inter agreement without topics\n",
    "#============Kappa without topics============================\n",
    "Kappa_trueLabelsWithoutTopics=[]\n",
    "\n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+(kappa_agree[j])\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " Kappa_trueLabelsWithoutTopics.append(truelabel)\n",
    "    \n",
    "#==============End Kappa without topics=================================\n",
    "\n",
    "#====================interagreement then reliability==================\n",
    "annt_tpc2=np.full((nb_annotators,np_topics),1.0)\n",
    "for j in range(0,nb_annotators):\n",
    "  if (annt_responses[j][i]==trueLabelsWithoutTopics[i]) :\n",
    "          for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc2[j,c]=round(annt_tpc2[j,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "  else:\n",
    "         for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                annt_tpc2[j,c]=round(annt_tpc2[j,c]-(tweets_topics[i,c]/topics[c]),4)\n",
    "\n",
    "trueLabels_agree_rel=[]\n",
    " \n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+(agree[j]*annt_tpc2[j,c])\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " trueLabels_agree_rel.append(truelabel)\n",
    " \n",
    "#====================End intr agreement then reliability=========================\n",
    "\n",
    "#====================Kappa interagreement then reliability==================\n",
    "Kappa_annt_tpc2=np.full((nb_annotators,np_topics),1.0)\n",
    "for j in range(0,nb_annotators):\n",
    "  if (annt_responses[j][i]==Kappa_trueLabelsWithoutTopics[i]) :\n",
    "          for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                Kappa_annt_tpc2[j,c]=round(Kappa_annt_tpc2[j,c]+(tweets_topics[i,c]/topics[c]),4)\n",
    "  else:\n",
    "         for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "                Kappa_annt_tpc2[j,c]=round(Kappa_annt_tpc2[j,c]-(tweets_topics[i,c]/topics[c]),4)\n",
    "\n",
    "kappa_trueLabels_agree_rel=[]\n",
    " \n",
    "for i in range(0,np_tweets):\n",
    " highsim=0.0\n",
    " truelabel=0\n",
    " for label in range(1,nb_labels+1):\n",
    "     sim=0.0\n",
    "     for j in range(0,nb_annotators):\n",
    "         if annt_responses[j][i]==label:\n",
    "             for c in range(0,np_topics):\n",
    "              if tweets_topics[i,c]!=0.0:\n",
    "               sim=sim+(agree[j]*Kappa_annt_tpc2[j,c])\n",
    "     if highsim<sim:\n",
    "         truelabel=label\n",
    "         highsim=sim\n",
    " kappa_trueLabels_agree_rel.append(truelabel)\n",
    " \n",
    "#====================End Kappa intr agreement then reliability=========================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===========Majority Voting===============================\n",
    "majority_voting=[]\n",
    "for j in range(0,np_tweets):\n",
    "        high=0\n",
    "        s=0\n",
    "        for x in range(1,nb_labels+1):\n",
    "         s=0\n",
    "         for i in range(0,nb_annotators):\n",
    "            if annt_responses[i][j]==x:\n",
    "                s=s+1\n",
    "         if s>high:\n",
    "          high=s\n",
    "          majority=x\n",
    "        majority_voting.append(majority)  \n",
    "#===================End Majority Voting===Ù================================\n",
    "unbiased_annt_topics=np.full((nb_annotators,np_topics),' ')\n",
    "unbiased_annt_tpcs=np.full((nb_annotators,np_topics),' ')\n",
    "for i in range(0,nb_annotators):\n",
    "    for j in range(0,np_topics):\n",
    "        if annt_topics[i][j]-annt_topics.mean()>0.0:\n",
    "             unbiased_annt_topics[i][j]='r'\n",
    "        else:\n",
    "             unbiased_annt_topics[i][j]='u'\n",
    "        if annt_tpc[i][j]-annt_tpc.mean()>0.0:\n",
    "             unbiased_annt_tpcs[i][j]='r'\n",
    "        else:\n",
    "             unbiased_annt_tpcs[i][j]='u'\n",
    "for i in range(0,nb_annotators):\n",
    "   spammer=True\n",
    "   for j in range(0,np_topics):       \n",
    "       if unbiased_annt_topics[i][j]=='r':\n",
    "        spammer=False\n",
    "   if spammer==True:\n",
    "       for c in range(0,np_topics):\n",
    "           unbiased_annt_topics[i][c]='s'\n",
    "           \n",
    "for i in range(0,nb_annotators):\n",
    "   spammer=True\n",
    "   for j in range(0,np_topics):       \n",
    "       if unbiased_annt_tpcs[i][j]=='r':\n",
    "        spammer=False\n",
    "   if spammer==True:\n",
    "       for c in range(0,np_topics):\n",
    "           unbiased_annt_tpcs[i][c]='s'\n",
    "print('Real Reliability of annotators')\n",
    "print(unbiased_annt_topics)\n",
    "print('Estimated Reliability of annotators')\n",
    "print(unbiased_annt_tpcs)\n",
    "counter=0\n",
    "for i in range(0,nb_annotators):\n",
    "    for j in range(0,np_topics):\n",
    "        if unbiased_annt_tpcs[i][j]!=unbiased_annt_topics[i][j]:\n",
    "            counter=counter+1\n",
    "#========================accuracy======================================\n",
    "hits_algo1=0\n",
    "hits_MV=0\n",
    "hits_NMF=0\n",
    "hits_withoutTopics=0\n",
    "hits_Agre_rel=0\n",
    "hits_kappa=0\n",
    "hits_kappaWithoutTopics=0\n",
    "kappa_hits_Agre_rel=0\n",
    "\n",
    "for i in range(0,np_tweets):\n",
    "    if groundTruth[i]==trueLabels[i]:\n",
    "        hits_algo1=hits_algo1+1\n",
    "    if majority_voting[i]==groundTruth[i]:\n",
    "        hits_MV=hits_MV+1\n",
    "#    if  trueLabelsNMF[i]==groundTruth[i]:\n",
    "#         hits_NMF=hits_NMF+1\n",
    "    if  trueLabelsWithoutTopics[i]==groundTruth[i]:\n",
    "          hits_withoutTopics=hits_withoutTopics+1\n",
    "    if  trueLabels_agree_rel[i]==groundTruth[i]:\n",
    "          hits_Agre_rel=hits_Agre_rel+1\n",
    "    if  Kappa_trueLabels[i]==groundTruth[i]:\n",
    "          hits_kappa=hits_kappa+1\n",
    "    if  Kappa_trueLabelsWithoutTopics[i]==groundTruth[i]:\n",
    "          hits_kappaWithoutTopics=hits_kappaWithoutTopics+1\n",
    "    if  kappa_trueLabels_agree_rel[i]==groundTruth[i]:\n",
    "          kappa_hits_Agre_rel=kappa_hits_Agre_rel+1      \n",
    "print('number of tweets',np_tweets)\n",
    "print('number of topics',np_topics)\n",
    "print('number of annotators',nb_annotators)\n",
    "print('accuracy of inter-agreement with topics',hits_algo1/np_tweets,'missclasification: ',np_tweets-hits_algo1,'of',np_tweets)\n",
    "print('accuracy of Kappa inter-agreement topics',hits_kappa/np_tweets,'missclasification:',np_tweets-hits_kappa,'of',np_tweets) \n",
    "\n",
    "print('accuracy of inter-agreement without topics',hits_withoutTopics/np_tweets,'missclasification:',np_tweets-hits_withoutTopics,'of',np_tweets) \n",
    "print('accuracy of Kappa inter-agreement without topics',hits_kappaWithoutTopics/np_tweets,'missclasification:',np_tweets-hits_kappaWithoutTopics,'of',np_tweets) \n",
    "\n",
    "print('accuracy of inter-agreement then reliability',hits_Agre_rel/np_tweets,'missclasification: ',np_tweets-hits_Agre_rel,'of',np_tweets)\n",
    "print('accuracy of kappa inter-agreement then reliability',kappa_hits_Agre_rel/np_tweets,'missclasification: ',np_tweets-kappa_hits_Agre_rel,'of',np_tweets)\n",
    "\n",
    "print('accuracy of Majority Voting',hits_MV/np_tweets,'missclasification:',np_tweets-hits_MV,'of',np_tweets) \n",
    "print('accuracy of annotators reliability',((np_topics*nb_annotators)-counter)/(np_topics*nb_annotators),'missclasification:',counter,'of',(np_topics*nb_annotators)) \n",
    "\n",
    "#print('accuracy of NMF',hits_NMF/np_tweets,np_tweets-hits_NMF)        \n",
    "#======================================================================\n",
    "count=0\n",
    "for i in range(len(annt_responses)):\n",
    "    for j in range(len(annt_responses[i])):\n",
    "         if annt_responses[i][j]==0:\n",
    "             count=count+1\n",
    "print ('sparsity of the responses matrix',count/(np_tweets*nb_annotators),'empty:',count,'of ',np_tweets*nb_annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
